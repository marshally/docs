# Building for Public Cloud Environments 


As teams are building more and more for public cloud, there are some common non-functional requirements and considerations that are raised over and over. Instead of having each design have to go through these discussions and have an inconsistent set of questions and answers, this document proposes to provide good default questions and answers to the common questions that most services should ask and answer.

This document is *not* intended to be an exhaustive list of all possible requirements, not an ivory tower answer. We want to help our teams by giving clear and actionable questions and recommendations and considerations. Teams are expected to read this document and decide the applicability to their own problem. They are free to deviate from these questions and answers — however these defaults represent the views of the VAT and should be considered carefully - deviations will of course have to be highlighted in design reviews and debated. The spirit here is for strong recommendations to be made, and for teams to understand the goals. 

### **Scaling Models**

A primary concern to be considered in any design is the scale strategy that a team elects for their particular service. While there may be good reason to deviate from the recommendations below, they should be thoughtful deviations where the pros and cons are discussed up front.

**[Horizontal vs Vertical Scaling of Services](https://www.ibm.com/blogs/cloud-computing/2014/04/09/explain-vertical-horizontal-scaling-cloud/)** 

* Whenever possible we should favor building fixed sized units of work for a service, and then designing to scale those units horizontally. Using this strategy we can achieve great scale, but it does have several implications that teams must be aware of in order to be successful with it as it is the opposite of the current model running in the pods. The reason to avoid scaling vertically when possible is that as you scale up the memory or concurrent threads on a process, the behavior of that process actually changes its characteristics and leads to more complex testing scenarios to mimic in the labs. By keeping a uniform shape to the workload you can use few horizontally scaled systems and then be able to project load behaviors with greater confidence. Additionally Vertical scale has much lower limits than horizontal scale — as we move to a model of more multi-tenanted services Vertical scaling will limit the spread of that service. 
* However achieving Horizontal Scale generally requires services to be fully [stateless](https://en.wikipedia.org/wiki/Service_statelessness_principle) (or externally stored shared state) and they must be able to spin up new nodes quickly, both of which can be a challenge if the service isn't designed for Horizontal scale from day 1.

**Scaling Data Storing Services**

* [Multi-Leader Clusters](https://dzone.com/articles/application-clustering)
    * Clustering is a great HA technique, and can help spread a read-heavy load, but as you scale out a cluster you multiply the write load as the cluster members increase their communication. Additionally since all of the data is copied everywhere, you pay for many storage copies, and you must scale your cluster members Vertically if you start to hit memory limits. Clustering is more complex and can suffer from 'split brain' scenarios when the cluster members split into rings and share between themselves, but operational intervention is required to realign the data. We should favor not using vanilla clustering as it does not scale well enough for our needs in general. 
* [Sharded Multi-Leader Clusters](https://en.wikipedia.org/wiki/Shard_(database_architecture))
    * To solve the problems with standard clusters, we can introduce sharded clusters — not all of the data is stored in every member. Instead sharded clusters scale their workloads horizontally and use a replication factor deciding how wide to spread the data (replication factor of 3 is a good choice so that you can survive 2 faults). This has the benefits of scaling both the read and write loads while still being highly available. Sharded clusters suffer from the same complexities and change for split-brained scenarios, but are still a good choice for many of our use cases but are operationally expensive to keep healthy. ElasticSearch and Cassandra both are examples of high scale services that use this clustering strategy. 
* [Single Leader, Multiple Read-Replica Clusters](https://redis.io/topics/replication)
    * When we consider how information is spread across multiple instances of a service we need to understand both the read and write patterns.  A large number of use cases within our domains are read heavy while being light on writes. Examples include Account Manager (we check a users far more frequently then we change it) and Product Catalog (customers view products far more frequently than a merchant updates them).  As a general rule of thumb we consider designs where we maintain a Single Leader with Multiple Read-Replicas to be a preferred approach over the complexities of Concurrent Multi-Leader configurations. When a single master will not scale we prefer single Region Multi-Leader configurations with Multiple Read-Replicas over the complexity and cost of Multi-Region configurations. 
* [CQRS](https://martinfowler.com/bliki/CQRS.html) - Command Query Responsibility Segregation 
    * Another useful pattern to think about is understanding that sometimes trying to support both read and write off the same models and technology leads to scaling problems. To resolve this we build a pipeline that is optimized for ingestion, then background processes that prepare the data asynchronously for availability optimized for query patterns. This is a common pattern for search and analytics based query patterns where the write load is written to a durable store and then micro-batch jobs run frequently to prepare the data for use in queries — but both paths are optimized for their own purposes. Any complex query patterns or ingestion that requires transformations or analytics benefits from this pattern.
* [Micro-Scale Testing](https://www.upwork.com/hiring/development/6-types-testing-know-microservices-architecture/)
    * In order to really understand the scale characteristics of your system, you need to design Load and Stress Tests in order to prove your scale. Doing this at full system scale is a dark art that produces hard to interpret answers to hard to understand questions. Instead we should guide teams to approach this as scientific in nature. We should build a single-unit service at small scale, and then design our tests to push that service to capacity and record our metrics and logs for close examination. We want to push this to the saturation point so that we know our choke points for a single unit of work. Once we have this information, we can project our performance characteristics because if we are horizontally scaling the service, the throughput of the service should also scale relatively linearly. 

### **Data Access MODELS**

When designing a new service, understanding the data access patterns up front is very important - it should be a primary factor in creating a scalable design. Before starting, spend time understanding how the service will receive data and how it will provide data to its clients.

* Read-optimized vs Write-optimized
    * When preparing a model, always spend time estimating the ratio of read load to write load, and build some query and writing models so that you can understand the complexities involved in each. Too many indexes and a write load slows way down and memory and disk usage balloon. Too few indexes and the queries are slow and user access patterns are bad. When possible we should favor Eventually Consistent style writing. We should also pay attention to caching - caching a read-heavy load can save a tremendous amount of stress on a database.
* [Realtime Streaming](https://cloud.google.com/solutions/architecture/real-time-stream-processing-iot) vs [Batching](https://en.wikipedia.org/wiki/Batch_processing) vs [Micro-Batching](https://hortonworks.com/blog/apache-storm-design-pattern-micro-batching/)
    * Streaming realtime systems are cool! They also cost a lot to run at scale. Before undertaking a project that designs a realtime streaming system, make absolutely sure that this is a real requirement and not an assumption. Often times people think binary - realtime streaming or infrequent batching (hourly, daily). Batching systems are very efficient at doing work - they involve the economy of scale of doing operations and for their data volumes are very fast, but customers do not see their data until after the batch job completes and this may lead to data inconsistencies across services that are noticeable by customers. Realtime streaming makes the opposite tradeoff - it processes records as they come in to make data available to customers quickly, but it consumes the maximum resources to do so. There is a sweet spot in the middle - near-realtime micro-batching. By micro-batching you can dramatically reduce the computational cost of the streaming system, while allowing you the flexibility to choose the right batch time to have an acceptable customer experience - it can be a continuum tradeoff between performance and availability. And by making it configurable, you can easily adjust it as you learn more about each axis - for many things a 5 minute micro-batch is perfectly acceptable. It is our recommendation that you favor micro-batching and that you test different batch frequencies to get the best tradeoff of near-realtime and performance.

### **HA Models**

It should be relatively uncontroversial to say that our services need to be “Highly Available”... but what does that really mean for your team? There are various levels of [HA](https://www.ibm.com/developerworks/community/blogs/RohitShetty/entry/high_availability_cold_warm_hot?lang=en_us) and considering the place your service has in the dependency chains are important so that you can pick an appropriate HA design, and then provide those expectations to the consumers of your service so that they can react accordingly. Within your service you might elect different strategies depending on what you are running — stateless workloads for instance can easily be made active-active. Data services however consider a lot more effort to do that with.

* [Availability Zones](https://cloudacademy.com/blog/aws-regions-and-availability-zones-the-simplest-explanation-you-will-ever-find-around/)
    * The first thing that is important to understand about building for AWS and Heroku platforms, is that Availability Zones are the primary method by which we achieve HA. We want to deploy any of our services to have an N+2 configuration whenever possible to allow ourselves to survive a double-fault — that is any 2 zones might fail, and our service would continue to function. The reason to go with N+2 over N+1 is that we do not have a firm SLA to resolution - to have a single failure would eliminate all backup in an N+1 scenario - N+2 dramatically reduces the odds of catastrophic failure and is of negligible additional cost. 
    * We use Availability Zones for this instead of regions because of network latency - AZs are guaranteed to be network-close to each other, despite having separated power, cooling and fiber channels and being physically separated. Even as infrastructure improves there is significant cost of moving data between regions. Provided we can meet our HA needs within a single region it is appropriate to reduce the overall complexity and cost of our systems.
* Active-Active
    * The ideal answer to HA is to have multiple versions of a service running  seamlessly load balancing between them. The service then works in tandem and synchronizes its data seamlessly. These systems are beautiful to operate. We expect that most of our stateless services should be built this was by default. Whoever with a service responsible for Data Storage, it can cost quite a lot to build and they typically they suffer from the same operational complexities as MultiLeader Clusters. All of our stateless services should be built this way, and mission critical data-backed services may consider this as a strong recommendation. 
* Hot Standby
    * A strong case can be made for Hot Standby failover - you simply run a 'backup' copy of your service and live sync the data from the Active to the Hot Standby. When the health of the active becomes problematic, the traffic can be shifted to the Hot Standby. The downside to this approach is that the near realtime replication required can lead you to taking on the same operational costs as Active-Active but without the scale benefits of Multi-Leader. We would not recommend this model as compared to Active-Active or cheaper alternatives below. 
* Warm Standby
    * In Warm Standby you would run a backup copy of your service, but the data would be flushed from the active copy periodically into a known backup location. When failover occurs the Warm system reads in the state from the backup location and takes over servicing. This is far less expensive than Hot or Active-Active, but you still incur the cost of maintain a complete backup copy of your service somewhere else that sits idle and costs money while doing nothing. This is slightly less complex than Hot Standby, but does not provide a lot of benefits beyond Cold Standby.
* Cold Standby (aka Orchestration Controlled)
    * As Devops culture and Infrastructure-as-Code have become popular, it has become fast to create an exact identical configuration to your running service, and deploy it somewhere else on demand. Often this takes a single digit number of minutes to go from Cold to Active. This approach has the same advantages as Warm Standby, but where you only incur the costs when you need to failover. To accomplish this you must be disciplined in your approach to automation of configuration and deployment to ensure your orchestration process can easily spin up new services quickly and without human intervention. This is our default approach to HA if a service cannot be run Active-Active and is a very acceptable for anything that is not customer-visible or that runs asynchronously from customer interactions.
* [Chaos Engineering](https://principlesofchaos.org/)
    * Resiliency testing is a must - you are not HA if you haven't induced failures, documented your behaviors, and tuned your system accordingly. Embracing Chaos Engineering principles are vital — we should of course crawl and execute Chaos in pre-production and not production until we get good at it. However systems failures occur and if we fail to test those scenarios under realistic conditions, we haven't done our jobs.

### **DR Models**

Bad things happen and we need to be ready for it. However we have to balance likelihood, severity, and effort against acceptability of risk in order to mitigate and recover from bad events. The DR model we use needs to be something that we test. 

* [Automate everything](https://www.kovair.com/blog/what-is-devops-and-how-automation-helps-achieve-it/)
    * We should be guiding teams to make the only manual thing be pressing “I Approve” for a workflow. By fully automating all aspects of our systems, we ensure that we eliminate the biggest opportunity for error — the one that lies between the keyboard and the chair. Automation is hard, and we all know it, but failure to automate leads to issues later. Perfect DR requires us to be perfectly repeatable. The worst thing that can happen in a high stress scenario like a DR is for us to leave room for a human mistake to prolong or even derail the entire recovery and further prolong the disaster event.
* Versioned Artifact Deployment
    * Each of Infrastructure-Templates, Infrastructure Configuration, Application Code, Application Configuration and Data Definitions need to be deployed in an automated fashion as **[independently semantically versioned artifacts](https://semver.org/)** as opposed to bespoke processes. This separation assures that in a disaster we can reload exact system state from a hardened repository and speeds the recovery times and decreased the likelihood of recover-related failures.
* Regions
    * Attempting to build connectivity across regions leads to inconsistent latencies. Clustering across such an environment will lead to issues listed about with Multi-Leader configurations, but because of network instabilities and latencies over the WAN you will see increased rates - making them more operationally expensive to maintain. AWS has a good track record of region-wide outages being quite rare events, and only impacting a single region at a time when they do occur (and typically just 1 service in that region). 
    * This means that building an Active-Active system to try to mitigate this is not a good return on investment because of the cost and complexity of building and operating it. We should instead push our teams to focus on automation and backup, and then replicating the backups our of the running region so that should an outage occur, because our infrastructure configuration is a versioned artifact, we could spin up our service in a new region, running off the backups, and taking a relatively modest downtime.  
* [DR Testing](https://aws.amazon.com/disaster-recovery/)
    * If you do not test your DR specifically then it hasn't been proven to work and **cannot** be relied upon. You can adapt some of your HA testing to prove out your DR, but DR must be tested - and the more frequently it gets exercised the more confidence you can have in it. Ideally your DR processes will largely overlap with your deployment & upgrade processes so that they are exercised frequently.

### **Performance Models**

When designing any component, always make sure to understand and document the performance expectations on it, and then design and execute tests to prove that you met your goals. Simple systems are easier to performance tune than complex ones so by reducing your system to its most basic form, you can then decide on your strategies for resolving the bottlenecks that you identify using some of the following techniques.

* Cache Invalidation
    * One of the most effective means of speeding performance and reducing stress on a database is to introduce caching. Caching can be effective at different layers in different ways, so you need to plan carefully when introducing a new cache. Remember that cache-eviction planning is widely considered to be one of the [hardest problems in Computer Science](https://martinfowler.com/bliki/TwoHardThings.html) so please plan your invalidations accordingly. Work to understand the actual need of data readiness - is a TTL sufficient? Do you need realtime invalidation? How much lag is acceptable in your service? The most lag you can accept, the more effective your cache can be. 
* In-Memory Caching 
    * Using tools like [EhCache](http://www.ehcache.org/) offer your best performance benefit, but come with the burden of needing to be capped at a particular size to avoid memory overruns, can effect the resources available to the rest of your service, and require warming time when any instance restarts — if you choose this strategy you may need to implement your own cache-warming to avoid performance hits when new instances of your service spin up.
* Distributed Caching 
    * Using tools like [Redis](https://redis.io/) or [Memcached](https://memcached.org/) offer a grid of computational power outside of your instance. You pay a penalty per access in needing a network hop to read from your cache, but the cache warming problem becomes minimized as the cache grid should restart very infrequently, and because it is an externalized system you can scale it independently of your service and it doesn't consume the in-memory resources of your service. Distributed caching in most cases should be considered preferential to in-memory caching.
* Cache-Aside Models
    * The most common implementation of a cache will be a cache-aside model - the calling code explicitly queries the cache for existence of the data it is looking for, and on fault, queries the primary data source. This is the easiest to understand what is going on, but puts the burden on the caller to know about the cache and how it is used. It also means that changes to the cache require changes to the client code. Because the alternative can be very complicated to implement for only minor benefit, this is the recommended model for most caching.
* Cache-Through Models
    * A more sophisticated model would be for the data access tier itself to make the caching transparent. This is the strategy that AWS [DAX](https://aws.amazon.com/dynamodb/dax/) uses over top of [DynamoDB](https://aws.amazon.com/dynamodb/), and what [Hibernate](http://hibernate.org/) provides as well. It lightens the burden on the client caller as they no longer need to think about the caching (mostly) however this can be frustrating when the client code requires more control. Since for most use cases we expect the data layer to be owned by the service's team, its the same team needing to think about both, so we do not recommend this model except if the cache invalidation is exceedingly complex and needs to be controlled more carefully.  
* Pre-calculation
    * Teams should consider when they can pre-calculate and store a result for fast access rather than looking to scale out a service to handle request load on demand. There is a cost tradeoff that needs to be considered and teams should consider where such a pattern is cost effective.  Pre-calculation is often valuable when the results to be produced are countable (ie: fit within a reasonable amount of storage), when strong consistency is not required, and when the ratio of processing time to change rate is low. It is recommended that pre-calculation be prefe over dynamic calculation for any expensive queries.
* Geo-distribution
    * When it comes to understanding service interactions we should consider the distribution of our services across the globe. In AWS this would, for example, include how to manage requests to different regions.  There are a number of options here.
        * Route requests to a single region
        * Route requests to a region based on “tenant”
        * Route requests based on origin
    * The choice depends on the need of the service. Single region is the simplest and appropriate if a region has an acceptable level of redundancy (for example we consider AWS and Heroku Regions to have acceptable redundancy) AND if we can afford the latency of a request being served from this location. Routing based on tenant is the next simplest as it means that data only needs to reside within a single “Region”.  While we may deploy our software stack to multiple regions they are all independent copies of one another. Routing based on origin is the most complicated as it makes no assumption of the requestor and requires that all information be available in any region in which the request may be served. When considering this method we should as well consider “Cluster” model and determine if a Single-Region-Leader, Multi-Region-Read-Replica setup supports our needs — writes might all route to a single region but reads route based on origin. 
* CDN
    * Contrary to popular belief, Content Delivery Networks are not just for static content anymore. CDNs invest in creating geo-proximity nodes that our customs will send their requests to. Once inside the CDN boundary the CDN usually has highly optimized routes from their edge nodes to our Origin Servers (your service) — meaning that it has a fast-lane path from the edge to your service. Customers will see better performance on CDN-optimized links than they will moving through the public internet. While this is majorly true for AWS [CloudFront](https://aws.amazon.com/cloudfront/dynamic-content/). Additionally you can use more advanced CDN caching features to provide an increased level of caching at the edge to further improve the performance of your service.

### **Security Models**

In order to even think about running our workloads in a public cloud, we need to ensure that we are doing everything we can to protect our customer data — after all Trust is our number 1 priority. When considering designing a system, understand how you plan to secure it from the start. Consult with the Prod Sec and Public Cloud Sec teams from the start as input, and keep them involved at every step to ensure that you are building to common best practices. Things to keep in mind in your design:

* Encryption Everywhere
    * Encrypted communications is a non-negotiable requirement of all services we build. Encryption of data at rest is equally non-negotiable — both are easy requirements to satisfy and go a long way to improving our posture - you must not cut these corners, ever. 
    * Try not to store any [PII](https://en.wikipedia.org/wiki/Personally_identifiable_information) if you can avoid it — at worst try to pseudo-anonymize it with an un-reversible algorithm. You should strongly consider field-level encryption when storage cannot be avoided. While this can be tedious, it offers our best protection and should be considered for any PII being stored in public cloud infrastructure where risk of exposure exceeds that of owned data centers. 
* [Zero Trust](https://opensource.com/article/17/6/4-easy-ways-work-toward-zero-trust-security-model) / [Principle of Least Privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege)
    * In the past networks were considered a hard perimeter that could be relied upon - but those days are gone. Today we want to use software-defined routes to minimize available routes, but we cannot allow reachability to be sufficient to authorize - [mTLS](https://en.wikipedia.org/wiki/Mutual_authentication) should be the low bar for authorization and you should consider an additional checking (as appropriate) to ensure that the actual actor is authorized to perform the action he is attempting on the dataset he is working with — not just that the processes in question are permitted to talk to each other.
* [Defense in Depth](https://en.wikipedia.org/wiki/Defense_in_depth_(computing))
    * In this day and age we are seeing some of the biggest companies get breached. If we assume that a [nation-state level actor](https://www.baesystems.com/en/cybersecurity/feature/the-nation-state-actor) is coming after us, we must also assume that they will breach one or more of our security protections. The safest assumption we can make is that SOMEONE will break into your service in EVENTUALLY. If you start from those assumptions, instead of focusing on 'how can I prevent them getting in', and turn the question to 'how can I minimize the damage by maximize the effort for the bad-actor has to put in to get value out of each datagram they can extrude and make it harder for them to get a second valuable datagram', we can focus on layering our protections so that breach of any single piece of the system does not yield a treasure trove. 
* [Whitelisting](https://en.wikipedia.org/wiki/Whitelisting)
    * Favor the use of explicit allowances over implicit ones. Practice maintaining whitelists of permitted operations by permitted actors and extends to all interactions with the service you are building. Look for every avenue to increase the security posture with things we know we can count on — if we only expected certain users to access from certain systems, make use of that fact in your design by explicitly declaring it as a requirement.
* [TTL](https://en.wikipedia.org/wiki/Time_to_live)
    * For every piece of data we store, there should be a defined period of time that data should be living. Your service must conform to our GDPR product promises. Access rights granted to processes and users should likewise be time limited in their duration - secrets must be rotated on a defined period, which should also conform to security best practices. 

### **Monitoring Considerations**

To promote a true Devops Culture — which is really required to be effective running services at scale in the public cloud, teams need to know their metrics cold. These metrics should be part of the design from the start as they are the measures by which the service will be judged. Metrics should be enriched and enhanced over time, but they are the data-driven way that you will make decisions about the performance, scale, and reliability of your service.

* Limits and Alerts
    * Services must always have defined limits. For every consumable resource, the service must declare a limit for that resource and describe its behavior as the limit is approached and eventually reached. The design should declare the threshold at which alerts occur as the limits are approached. These thresholds must be tested and adjusted based on observed bottlenecks. Keep your metrics in front of your team so that everyone understand what they mean and why. Make a policy that every alert must be investigated with the output documented — do not accept noisy alerts. 
* Consuming Monitoring Data
    * Monitoring data for a service is best viewed in a single pain of glass, and reviewed by your team multiple times of data. The data should become so familiar to your team that a causal glance at the dashboards should be enough to trigger a 'something doesn't look right' reaction even when alerts have not been fired. Your team should be intimately familiar with its monitoring and metrics charts at various times of data and different days of the week. The best way to achieve this is to have metrics cycling on monitors in public locations that your team meets in, and to encourage conversation and investigation about unexpected spikes or drops.
* Heisenberg's Uncertainty Principle
    * Remember that monitoring hooks do take cycles from your service. Be thoughtful about how you collect your metrics or else the very act of instrumenting may change your performance shape. It is possible to thoughtlessly instrument can cause a workload to become must more expensive. 
* Custom Metrics Monitoring vs APM
    * Custom Metrics tooling allow you to define your own monitoring points and feed them to [statsd](https://github.com/etsy/statsd) who then sends them to [Graphite](https://graphiteapp.org/)/[Graphana](https://grafana.com/) where you can define your own thresholds, alerts, and dashboards for your team to track. It offers a great deal of control for your team. If your team well understands its domain, this can be your most efficient path. 
    * Application Performance Monitoring (APM) tooling like [AppDyanmics](https://www.appdynamics.com/), [DataDog](https://www.datadoghq.com/), or [New Relic](https://newrelic.com/) offer a deeper level of interaction where in addition to taking statsd feeds, they also plug into the JVM and instrument it to record things that your team might not have planned for. Generally this increased visibility comes at a minor performance penalty and they can become difficult to setup and configure. If you have a problem space where you are less confident of your boundaries or are dealing with a performance or scale issue that you cannot seem to understand, this is often the best choice of tooling. 

### **Logging Considerations**

Logs are the lifeblood of a SaaS system. Your team should have strong understanding of all of the logs their service emits and why. You should be able to use your logs alone to diagnose and troubleshoot most issues with your service. Often you will not have access at all to the running system so further collection of data may be hard or even impossible. To do that requires careful planning. 

* Events vs Logs
    * Logs are messages that are emitted that are useful to Salesforce employees. Events are logs of such meaning to the customers that we want to share them with them. When creating logs we should always consider which of our logs should be elevated to Event status and shared with customers. No one except the service owner is in a position to really understand the log messages in a way that they can make that determination. When planning for your service you should include explicit discussion and documentation of what your logs mean, and which log messages are Events to be shared with customers.
* Distributed Transaction Tracing
    * In a microservices architecture you might see several different services cooperating in order to satisfy a single customer's request. In order to understand and effectively track a single customer interaction across potentially many services, we need to use a method to tag all of the interactions with a single traceable identifier. Salesforce has chosen [Zipkin](https://zipkin.io/) as our framework for doing this - all services must include Zipkin trasaction ids in their log format.
* Spam
    * There is nothing worse than being in the heat of a customer impacting issue and your team starts chasing log file entries only to keep finding out that they are chasing a red-herring — that the log message is 'benign'. Firstly your teams should have enough knowledge of what is normal and not normal because they should be looking at the logs of normal systems every day. However when we come across 'benign' log messages, we need to remove them for the good of everyone involved. Noisy logs lead to wasted time in the heat of a customer issue, and they make it hard to see the real problems in the system. A healthy system should have no noise during normal operations, sporadic useful logging as we approach unexpected situations and obvious indicators of failure during true failure modes. To get this right you have to both work with the logs all the time and you have to practice log grooming - remove unhelpful logging when you find it or adjust its level. 
* Debuggers
    * Try debugging your system with just the logs most of the time - consider reserving your debugger usage for difficult problems. By getting into the practice of debugging using your logs, you will naturally write more useful and better log messaging. By incorporating it into your daily process you will be better able to feel intimately familiar with your logs and it will help your team resolve customer issues more rapidly.



